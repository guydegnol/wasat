{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xrRU-Njoh5hp",
    "outputId": "a436220c-26eb-4042-821c-acf34b165091"
   },
   "outputs": [],
   "source": [
    "#filedrive=\"/content/drive/My Drive/0Satellite_Images_Unsupervised/MNIST_PyTorch\"\n",
    "#from google.colab import drive\n",
    "#drive.mount('/content/drive')\n",
    "filedrive=\"data/0Satellite_Images_Unsupervised/MNIST_PyTorch\"\n",
    "\n",
    "import torch, torchvision\n",
    "from torch import nn, optim\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 103
    },
    "id": "zCnYpfgJtS-I",
    "outputId": "de7cb47d-8189-41db-fbd8-85dd2a8a6863"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nclass Encoder(nn.Module):\\n    def __init__(self, bands_nb, patch_size):\\n        input_size = (bands_nb, patch_size, patch_size)\\n\\n        super(Encoder, self).__init__()\\n\\n        # Stage 1\\n        # Feature extraction\\n        self.conv11 = nn.Conv2d(bands_nb, 32, kernel_size=3, padding=1)\\n        self.bn11 = nn.BatchNorm2d(32)\\n        self.conv12 = nn.Conv2d(32, 32, kernel_size=3, padding=1)\\n        self.bn12 = nn.BatchNorm2d(32)\\n        self.conv13 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\\n        self.bn13 = nn.BatchNorm2d(64)\\n        self.conv14 = nn.Conv2d(64, 64, kernel_size=3, padding=1)\\n        self.bn14 = nn.BatchNorm2d(64)\\n        self.activation1 = nn.ReLU()\\n        self.activation2 = nn.Sigmoid()\\n        self.l2norm = L2()\\n\\n\\n    def forward(self, x):\\n        # Stage 1\\n        x11 = F.relu(self.bn11(self.conv11(x)))\\n        x12 = F.relu(self.bn12(self.conv12(x11)))\\n        x13 = F.relu(self.bn13(self.conv13(x12)))\\n        x14 = \\n        size14 = x14.size()\\n        x14_ = x14.view(size14[0], size14[1], size14[2]*size14[3])\\n        x14_ = F.normalize(x14_, p=2, dim=2)\\n        encoded = x14_.view(size14[0], size14[1], size14[2], size14[3])\\n\\n\\n        return encoded\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, bands_nb, patch_size):\n",
    "        input_size = (bands_nb, patch_size, patch_size)\n",
    "\n",
    "        super(Encoder, self).__init__()\n",
    "\n",
    "        # Stage 1\n",
    "        # Feature extraction\n",
    "        self.conv11 = nn.Conv2d(bands_nb, 32, kernel_size=3, padding=1)\n",
    "        self.bn11 = nn.BatchNorm2d(32)\n",
    "        self.conv12 = nn.Conv2d(32, 32, kernel_size=3, padding=1)\n",
    "        self.bn12 = nn.BatchNorm2d(32)\n",
    "        self.conv13 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "        self.bn13 = nn.BatchNorm2d(64)\n",
    "        self.conv14 = nn.Conv2d(64, 64, kernel_size=3, padding=1)\n",
    "        self.bn14 = nn.BatchNorm2d(64)\n",
    "        self.activation1 = nn.ReLU()\n",
    "        self.activation2 = nn.Sigmoid()\n",
    "        self.l2norm = L2()\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Stage 1\n",
    "        x11 = F.relu(self.bn11(self.conv11(x)))\n",
    "        x12 = F.relu(self.bn12(self.conv12(x11)))\n",
    "        x13 = F.relu(self.bn13(self.conv13(x12)))\n",
    "        x14 = \n",
    "        size14 = x14.size()\n",
    "        x14_ = x14.view(size14[0], size14[1], size14[2]*size14[3])\n",
    "        x14_ = F.normalize(x14_, p=2, dim=2)\n",
    "        encoded = x14_.view(size14[0], size14[1], size14[2], size14[3])\n",
    "\n",
    "\n",
    "        return encoded\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "2eA45Tm6jLl3"
   },
   "outputs": [],
   "source": [
    "class Classifier(nn.Module):\n",
    "    def __init__(self, bands_nb, patch_size):\n",
    "\n",
    "        input_size = (bands_nb, patch_size, patch_size)\n",
    "      \n",
    "        super().__init__()\n",
    "        self.layer1 = nn.Linear(784, 20)\n",
    "        self.act1 = nn.ReLU()\n",
    "        self.layer2 = nn.Linear(20, 10)\n",
    "        self.layer3 = nn.Linear(784*32, 10)\n",
    "\n",
    "        self.conv11 = nn.Conv2d(bands_nb, 32, kernel_size=3, padding=1)\n",
    "        self.bn11 = nn.BatchNorm2d(32)\n",
    "        self.conv12 = nn.Conv2d(32, 32, kernel_size=3, padding=1)\n",
    "        self.bn12 = nn.BatchNorm2d(32)\n",
    "        self.conv13 = nn.Conv2d(32, 32, kernel_size=3, padding=1)\n",
    "        self.conv14 = nn.Conv2d(32, 1, kernel_size=3, padding=1)\n",
    "\n",
    "        self.activation1 = nn.ReLU()\n",
    "        self.activation2 = nn.Sigmoid()\n",
    "#        self.pool = nn.MaxPool2d(2, 2)\n",
    "#        self.l2norm = L2()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x=F.relu(self.conv11(x)) #        x = self.pool(nn.functional.relu(self.conv1(x)))\n",
    "        x=F.relu(self.conv12(x))\n",
    "         # flatten all dimensions except batch\n",
    "#        x = F.relu(self.layer3(x))\n",
    "        x=F.relu(self.conv13(x))\n",
    "        return self.conv14(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "FD8P5YtctWxU"
   },
   "outputs": [],
   "source": [
    "#  use gpu if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "bands_nb=1\n",
    "patchsize=1\n",
    "# load model to the specified device, either gpu or cpu\n",
    "model = Classifier(bands_nb,patchsize).to(device)\n",
    "\n",
    "# create an optimizer object\n",
    "# Adam optimizer with learning rate 1e-3\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "# mean-squared error loss\n",
    "#criterion = nn.CrossEntropyLoss()\n",
    "criterion = nn.MSELoss()#criterion(outputs, batch_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bjYpajkh9x9o",
    "outputId": "f4f0bd94-7434-4598-b0d5-536d898d4b57"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1          [128, 32, 28, 28]             320\n",
      "            Conv2d-2          [128, 32, 28, 28]           9,248\n",
      "            Conv2d-3          [128, 32, 28, 28]           9,248\n",
      "            Conv2d-4           [128, 1, 28, 28]             289\n",
      "================================================================\n",
      "Total params: 19,105\n",
      "Trainable params: 19,105\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.38\n",
      "Forward/backward pass size (MB): 74.27\n",
      "Params size (MB): 0.07\n",
      "Estimated Total Size (MB): 74.72\n",
      "----------------------------------------------------------------\n",
      "Classifier(\n",
      "  (layer1): Linear(in_features=784, out_features=20, bias=True)\n",
      "  (act1): ReLU()\n",
      "  (layer2): Linear(in_features=20, out_features=10, bias=True)\n",
      "  (layer3): Linear(in_features=25088, out_features=10, bias=True)\n",
      "  (conv11): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (bn11): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (conv12): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (bn12): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (conv13): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (conv14): Conv2d(32, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (activation1): ReLU()\n",
      "  (activation2): Sigmoid()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "from torchsummary import summary\n",
    "summary(model,(1,28,28),batch_size=128, device='cuda')\n",
    "print(model)\n",
    "#print(((3*3+1)*32*31))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "UGI9RjsGh7kZ"
   },
   "outputs": [],
   "source": [
    "transform = torchvision.transforms.Compose([torchvision.transforms.ToTensor()])\n",
    "\n",
    "train_dataset = torchvision.datasets.MNIST(\n",
    "    root=filedrive, train=True, transform=transform, download=True)\n",
    "\n",
    "test_dataset = torchvision.datasets.MNIST(\n",
    "    root=filedrive, train=False, transform=transform, download=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "4AlykiXIlDpy"
   },
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=128, shuffle=True, num_workers=2, pin_memory=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=32, shuffle=False, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QpVeFNrNlPri",
    "outputId": "2b65c9bd-7939-4f01-bd79-11daee79b75d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch : 1/5, loss = 0.002411\n",
      "epoch : 2/5, loss = 0.000029\n",
      "epoch : 3/5, loss = 0.000017\n",
      "epoch : 4/5, loss = 0.000016\n",
      "epoch : 5/5, loss = 0.000012\n"
     ]
    }
   ],
   "source": [
    "epochs = 5\n",
    "from tqdm import tqdm\n",
    "\n",
    "pbar = tqdm([\"a\", \"b\", \"c\", \"d\"])\n",
    "for char in pbar:\n",
    "    sleep(0.25)\n",
    "    pbar.set_description(\"Processing %s\" % char)\n",
    "    \n",
    "pbar = tqdm(range(epochs))\n",
    "for epoch in pbar:\n",
    "    loss = 0\n",
    "    for batch_features, batch_labels in train_loader:\n",
    "        # reshape mini-batch data to [N, 784] matrix\n",
    "        # load it to the active device\n",
    "        #batch_features = batch_features.view(-1, 784).to(device)\n",
    "        batch_features = batch_features.to(device)\n",
    "        # reset the gradients back to zero\n",
    "        # PyTorch accumulates gradients on subsequent backward passes\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # compute reconstructions\n",
    "        outputs = model(batch_features)\n",
    "        #outputs = outputs.view(-1).to(device)\n",
    "\n",
    "        # compute training reconstruction loss\n",
    "\n",
    "#        train_loss = criterion(outputs, batch_labels.to(device))\n",
    "        train_loss = criterion(outputs, batch_features)\n",
    "\n",
    "        # compute accumulated gradients\n",
    "        train_loss.backward()\n",
    "\n",
    "        # perform parameter update based on current gradients\n",
    "        optimizer.step()\n",
    "\n",
    "        # add the mini-batch training loss to epoch loss\n",
    "        loss += train_loss.item()\n",
    "\n",
    "    # compute the epoch training loss\n",
    "    loss = loss / len(train_loader)\n",
    "\n",
    "    # display the epoch training loss\n",
    "    pbar.set_description(\"epoch : {}/{}, loss = {:.6f}\".format(epoch + 1, epochs, loss))\n",
    "    print(\"epoch : {}/{}, loss = {:.6f}\".format(epoch + 1, epochs, loss))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 380
    },
    "id": "MZDhEn06lW4c",
    "outputId": "39443d74-04b2-4f35-fbff-9c19ce16d67d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset MNIST\n",
      "    Number of datapoints: 10000\n",
      "    Root location: data/0Satellite_Images_Unsupervised/MNIST_PyTorch\n",
      "    Split: Test\n",
      "    StandardTransform\n",
      "Transform: Compose(\n",
      "               ToTensor()\n",
      "           )\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (28) must match the size of tensor b (32) at non-singleton dimension 2",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-d92a1fca6af9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredicted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0mtotal\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m     \u001b[0mcorrect\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mpredicted\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Accuracy of the network on the 10000 test images: %d %%'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m100\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mcorrect\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mtotal\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (28) must match the size of tensor b (32) at non-singleton dimension 2"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "total = 0\n",
    "correct = 0\n",
    "print(test_loader.dataset)\n",
    "for batch_features, labels in test_loader:\n",
    "#    batch_features = batch_features.view(-1, 784).to(device)  \n",
    "    batch_features = batch_features.to(device)\n",
    "    outputs = model(batch_features)\n",
    "    #print(outputs.shape)\n",
    "\n",
    "    #print('IMAGE')\n",
    "\n",
    "    #plt.imshow(batch_features[0].cpu().detach().view(28, 28).numpy())\n",
    "    #plt.show()     \n",
    "\n",
    "    #print(np.argmax(outputs[0].cpu().detach().numpy()))   \n",
    "\n",
    "    _, predicted = torch.max(outputs.data, 1)\n",
    "    total += labels.size(0)\n",
    "    correct += (predicted.cpu() == labels).sum().item()\n",
    "\n",
    "print('Accuracy of the network on the 10000 test images: %d %%' % (100 * correct / total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 198
    },
    "id": "UxhZWyNSlenk",
    "outputId": "f0834000-5054-40fe-9595-4b1f273402f2"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from numpy import array\n",
    "print(batch_features.shape)\n",
    "#print(batch_features[0,0,:,:])\n",
    "print(labels[0])\n",
    "n=16\n",
    "numpy_data=batch_features[0:n,0,:,:].cpu().detach().view(n,28, 28).numpy()\n",
    "outputs = model(batch_features[0:n,:,:,:])\n",
    "#_, predicted = torch(outputs.data, 1)\n",
    "pred=outputs.cpu().detach().numpy()\n",
    "#print(\"predicted=\",predicted.cpu().detach().numpy())\n",
    "print(pred.shape)\n",
    "#fig, axs = plt.subplots(nrow, nrow, figsize=(20,20))\n",
    "\n",
    "fig=plt.figure(n, figsize=(10,10))\n",
    "for i in range(n):\n",
    "  fig.add_subplot(1, n,  1+i) \n",
    "  plt.imshow(numpy_data[i,:,:],label=\"predicted=\"+str(pred[i])) \n",
    "#  axs[i1,i2].set_title('Th={Theta_title:.2f}  E={E_title:.3f}'.format(Theta_title=Theta,E_title=E), fontsize=12)    \n",
    "\n",
    "plt.show()\n",
    "\n",
    "fig=plt.figure(n, figsize=(10,10))\n",
    "for i in range(n):\n",
    "  fig.add_subplot(1, n,  1+i) \n",
    "  plt.imshow(pred[i,0,:,:],label=\"predicted=\"+str(pred[i])) \n",
    "\n",
    "#  axs[i1,i2].set_title('Th={Theta_title:.2f}  E={E_title:.3f}'.format(Theta_title=Theta,E_title=E), fontsize=12)    \n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VppZmZ84In9_"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "AE_conv_MNIST_Pytorch.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "mlapp_py3.6",
   "language": "python",
   "name": "mlapp_py3.6"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
